{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "SFClpvPNcvH4"
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "oWE5goj6mnGQ"
   },
   "outputs": [],
   "source": [
    "NUM_CLASSES = 10\n",
    "WIDTH = 32\n",
    "NUM_CHANNELS = 3\n",
    "NUM_TRAIN = 50000\n",
    "NUM_TEST = 10000\n",
    "NUM_DEV = 100\n",
    "\n",
    "DATASET = \"cifar10\"\n",
    "BASE_DIR = \"\"  # set to whatever directory you are working/saving in\n",
    "BATCH_SIZE = 128  # increase batch_size for faster training (if your file does not crash)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FqNEYLvZKW2T"
   },
   "source": [
    "## Data loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "dmyOUizW9Kde"
   },
   "outputs": [],
   "source": [
    "def _normalize(X):\n",
    "  assert X.dtype == np.uint8\n",
    "  X = X.astype(np.float64)\n",
    "  X /= 255\n",
    "  return X\n",
    "\n",
    "def get_one_hot(targets, nb_classes):\n",
    "  res = np.eye(nb_classes)[np.array(targets).reshape(-1)]\n",
    "  return res.reshape(list(targets.shape)+[nb_classes])\n",
    "\n",
    "def load_standard_cifar10():\n",
    "  (X_train, Y_train), (X_validation, Y_validation) = tf.keras.datasets.cifar10.load_data()\n",
    "  X_train = X_train.reshape(X_train.shape[0], WIDTH, WIDTH, NUM_CHANNELS)\n",
    "  X_validation = X_validation.reshape(X_validation.shape[0], WIDTH, WIDTH, NUM_CHANNELS)\n",
    "\n",
    "  X_train = _normalize(X_train)\n",
    "  X_validation = _normalize(X_validation)\n",
    "\n",
    "  Y_train = Y_train.astype(np.int32)\n",
    "  Y_validation = Y_validation.astype(np.int32)\n",
    "\n",
    "  return X_train, Y_train, X_validation, Y_validation\n",
    "\n",
    "def load_cifar10_train_dev(num_dev=100):\n",
    "  # randomly select and fixed for future (tracin-like strategy but their indices available only for mnist)\n",
    "  # selected_dev = np.random.randint(0, X_validation.shape[0], num_dev)\n",
    "  selected_dev = [5214, 2304, 5947, 9428, 2717, 8296, 7736, 8291, 5235, 54,\n",
    "                  7499, 9590, 3675, 1932, 6646, 8719, 6484, 6306, 3066, 2442,\n",
    "                  6106, 1949, 4320,  541, 1318, 5967, 2773, 3847, 1152, 9937,\n",
    "                  7469, 5982, 7644, 5820, 8152, 9518,  601, 3953, 4931, 1924,\n",
    "                  5342, 5467, 6718, 6779, 2860, 2440, 5480, 1178,  222, 7909,\n",
    "                  6394, 3511, 8729, 6261, 7192, 9453, 5257, 9077, 6419, 3280,\n",
    "                  3725, 3601, 8174, 5703, 4954, 9536, 4783, 2234, 7365, 2405,\n",
    "                  3073, 2780, 7461, 3525, 7573, 6764, 9962, 7527,  992,  315,\n",
    "                  6260, 9061,  592, 8003, 7594, 1930, 7215, 5124, 7531, 9471,\n",
    "                  2824, 3533, 6062, 3946, 5246, 4440,  414, 3572, 4899, 884]\n",
    "  X_train, Y_train, X_validation, Y_validation = load_standard_cifar10()\n",
    "  X_dev = X_validation[selected_dev]\n",
    "  Y_dev = Y_validation[selected_dev]\n",
    "  return X_train, Y_train, X_dev, Y_dev"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "background_save": true
    },
    "id": "_q_HiHPqg94u"
   },
   "outputs": [],
   "source": [
    "X_train, Y_train, X_test, Y_test = load_standard_cifar10()\n",
    "_, _, X_dev, Y_dev = load_cifar10_train_dev()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "background_save": true
    },
    "id": "OazEet6QmJaQ"
   },
   "outputs": [],
   "source": [
    "Y_train = np.squeeze(np.array(Y_train))\n",
    "Y_test = np.squeeze(np.array(Y_test))\n",
    "Y_dev = np.squeeze(np.array(Y_dev))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "background_save": true
    },
    "id": "jjRtFp3qlw8W",
    "outputId": "90c013a9-3475-40cd-c15d-99ae8293cbac"
   },
   "outputs": [],
   "source": [
    "print(X_train.shape, X_test.shape, X_dev.shape, Y_train.shape, Y_test.shape, Y_dev.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "c1aslxI4_kxI"
   },
   "source": [
    "## Model training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "background_save": true
    },
    "id": "4wtzw_Jq_lu1"
   },
   "outputs": [],
   "source": [
    "def dnn_custom(inp_dim, out_dim, dnn='resnet50', train_full=False, weights='imagenet', dropout_pct=0.25, use_upsampling=False):\n",
    "    inp_dim_orig = inp_dim\n",
    "    if use_upsampling:\n",
    "      inp_dim = (224,224,3)\n",
    "    if dnn=='resnet50':\n",
    "      dnn_model = tf.keras.applications.ResNet50(weights=weights, include_top=False, input_shape=inp_dim)\n",
    "    elif dnn=='mobilenet':\n",
    "      dnn_model = tf.keras.applications.MobileNet(weights=weights, include_top=False, input_shape=inp_dim)\n",
    "    elif dnn=='mobilenetv3':\n",
    "      dnn_model = tf.keras.applications.MobileNetV3Small(weights=weights, include_top=False, input_shape=inp_dim)\n",
    "    elif dnn=='efficientnetb0':\n",
    "      dnn_model = tf.keras.applications.EfficientNetB0(weights=weights, include_top=False, input_shape=inp_dim)\n",
    "    elif dnn=='efficientnetb3':\n",
    "      dnn_model = tf.keras.applications.EfficientNetB3(weights=weights, include_top=False, input_shape=inp_dim)\n",
    "    elif dnn=='efficientnetv2':\n",
    "      dnn_model = tf.keras.applications.EfficientNetV2L(weights=weights, include_top=False, input_shape=inp_dim)\n",
    "    elif dnn=='convnext':\n",
    "      dnn_model = tf.keras.applications.ConvNeXtBase(weights=weights, include_top=False, input_shape=inp_dim)\n",
    "    elif dnn=='xception':\n",
    "      dnn_model = tf.keras.applications.Xception(weights=weights, include_top=False, input_shape=inp_dim)\n",
    "\n",
    "    if not train_full:\n",
    "      for layer in dnn_model.layers:\n",
    "          if isinstance(layer, tf.keras.layers.BatchNormalization):\n",
    "              layer.trainable = True\n",
    "          else:\n",
    "              layer.trainable = False\n",
    "\n",
    "    model = tf.keras.models.Sequential()\n",
    "    model.add(tf.keras.layers.Input(shape=inp_dim_orig))\n",
    "    if use_upsampling & inp_dim_orig[0] != 224:\n",
    "      model.add(tf.keras.layers.UpSampling2D(size=(224/inp_dim_orig[0], 224/inp_dim_orig[0]), interpolation='bilinear'))\n",
    "    model.add(dnn_model)\n",
    "    model.add(tf.keras.layers.GlobalAveragePooling2D())\n",
    "    model.add(tf.keras.layers.Dropout(dropout_pct))\n",
    "    model.add(tf.keras.layers.Dense(out_dim*16, activation='relu'))\n",
    "    model.add(tf.keras.layers.BatchNormalization())\n",
    "    model.add(tf.keras.layers.Dense(out_dim, activation='softmax'))\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "xu76b-ds_l0d",
    "outputId": "fe108287-da7c-4b51-c069-0eb5a261eeff"
   },
   "outputs": [],
   "source": [
    "BASE_DNN = 'resnet50'\n",
    "EPOCHS = 1000\n",
    "saved_epochs = [1, 5, 10, 30, 50, 75, 100, 200, 300, 400, 500, 600, 700, 750, 800, 900, 1000]\n",
    "\n",
    "start_epoch = 0\n",
    "for e in saved_epochs:\n",
    "    try:\n",
    "        clf = tf.keras.models.load_model(f\"{BASE_DIR}/{DATASET}_{BASE_DNN}_{e}e_{pct_poison}dp.h5\")\n",
    "        start_epoch = e\n",
    "    except:\n",
    "        break\n",
    "    \n",
    "if start_epoch == 0:\n",
    "    clf = dnn_custom(inp_dim=(32,32,3), out_dim=NUM_CLASSES, dnn=BASE_DNN, dropout_pct=0, train_full=False, use_upsampling=True)\n",
    "    clf.compile(\n",
    "        optimizer='Adam', \n",
    "        loss=tf.keras.losses.SparseCategoricalCrossentropy(), \n",
    "        metrics=['accuracy', tf.keras.metrics.SparseTopKCategoricalAccuracy(k=5)]\n",
    "    )\n",
    "\n",
    "print(start_epoch)\n",
    "for i in range(start_epoch+1,EPOCHS+1):\n",
    "  if (i%10) == 0:\n",
    "    print(i)\n",
    "  clf.fit(X_train, Y_train, epochs=1, batch_size=BATCH_SIZE, validation_data=(X_test, Y_test), verbose=1)\n",
    "  if i in saved_epochs:\n",
    "    clf.save(f\"{BASE_DIR}/{DATASET}_{BASE_DNN}_{i}e_{pct_poison}dp.h5\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exploring alternate models (DNN architectures)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_dnn_list = ['efficientnetv2'] #, 'efficientnetb0', 'efficientnetb3', 'convnext']\n",
    "EPOCHS = 100\n",
    "saved_epochs = [1, 5, 10, 20, 30, 40, 50, 60, 70, 80, 90, 100]\n",
    "\n",
    "for dnn in base_dnn_list:\n",
    "    print(dnn)\n",
    "    start_epoch = 0\n",
    "    for e in saved_epochs:\n",
    "        try:\n",
    "            # custom layer object \n",
    "            if dnn == 'convnext':\n",
    "                clf = dnn_custom(inp_dim=(32,32,3), out_dim=NUM_CLASSES, dnn=dnn, dropout_pct=0, train_full=False, use_upsampling=True)\n",
    "                clf.compile(\n",
    "                    optimizer='Adam', \n",
    "                    loss=tf.keras.losses.SparseCategoricalCrossentropy(), \n",
    "                    metrics=['accuracy', tf.keras.metrics.SparseTopKCategoricalAccuracy(k=5)]\n",
    "                )\n",
    "                clf.load_weights(f\"{BASE_DIR}/{DATASET}_{dnn}_{e}e_{pct_poison}dp.h5\")\n",
    "            else:\n",
    "                clf = tf.keras.models.load_model(f\"{BASE_DIR}/{DATASET}_{dnn}_{e}e_{pct_poison}dp.h5\")\n",
    "            start_epoch = e\n",
    "        except:\n",
    "            break\n",
    "\n",
    "    if start_epoch == 0:\n",
    "        clf = dnn_custom(inp_dim=(32,32,3), out_dim=NUM_CLASSES, dnn=dnn, dropout_pct=0, train_full=False, use_upsampling=True)\n",
    "        clf.compile(\n",
    "            optimizer='Adam', \n",
    "            loss=tf.keras.losses.SparseCategoricalCrossentropy(), \n",
    "            metrics=['accuracy', tf.keras.metrics.SparseTopKCategoricalAccuracy(k=5)]\n",
    "        )\n",
    "\n",
    "    print(start_epoch)\n",
    "    for i in range(start_epoch+1,EPOCHS+1):\n",
    "      if (i%10) == 0:\n",
    "        print(i)\n",
    "      clf.fit(X_train, Y_train, epochs=1, batch_size=BATCH_SIZE, validation_data=(X_test, Y_test), verbose=1)\n",
    "      if i in saved_epochs:\n",
    "        clf.save(f\"{BASE_DIR}/{DATASET}_{dnn}_{i}e_{pct_poison}dp.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "machine_shape": "hm",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "mdl",
   "language": "python",
   "name": "mdl"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
