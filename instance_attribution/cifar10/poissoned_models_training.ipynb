{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "SFClpvPNcvH4"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-01-20 21:34:12.250808: E tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:9342] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-01-20 21:34:12.250853: E tensorflow/compiler/xla/stream_executor/cuda/cuda_fft.cc:609] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-01-20 21:34:12.250871: E tensorflow/compiler/xla/stream_executor/cuda/cuda_blas.cc:1518] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-01-20 21:34:12.257341: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "oWE5goj6mnGQ"
   },
   "outputs": [],
   "source": [
    "NUM_CLASSES = 10\n",
    "WIDTH = 32\n",
    "NUM_CHANNELS = 3\n",
    "NUM_TRAIN = 50000\n",
    "NUM_TEST = 10000\n",
    "NUM_DEV = 100\n",
    "\n",
    "DATASET = \"cifar10\"\n",
    "BASE_DIR = \"\"  # set to whatever directory you are working/saving in\n",
    "BATCH_SIZE = 128  # increase batch_size for faster training (if your file does not crash)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FqNEYLvZKW2T"
   },
   "source": [
    "## Data loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "dmyOUizW9Kde"
   },
   "outputs": [],
   "source": [
    "def _normalize(X):\n",
    "  assert X.dtype == np.uint8\n",
    "  X = X.astype(np.float64)\n",
    "  X /= 255\n",
    "  return X\n",
    "\n",
    "def get_one_hot(targets, nb_classes):\n",
    "  res = np.eye(nb_classes)[np.array(targets).reshape(-1)]\n",
    "  return res.reshape(list(targets.shape)+[nb_classes])\n",
    "\n",
    "def load_standard_cifar10():\n",
    "  (X_train, Y_train), (X_validation, Y_validation) = tf.keras.datasets.cifar10.load_data()\n",
    "  X_train = X_train.reshape(X_train.shape[0], WIDTH, WIDTH, NUM_CHANNELS)\n",
    "  X_validation = X_validation.reshape(X_validation.shape[0], WIDTH, WIDTH, NUM_CHANNELS)\n",
    "\n",
    "  X_train = _normalize(X_train)\n",
    "  X_validation = _normalize(X_validation)\n",
    "\n",
    "  Y_train = Y_train.astype(np.int32)\n",
    "  Y_validation = Y_validation.astype(np.int32)\n",
    "\n",
    "  return X_train, Y_train, X_validation, Y_validation\n",
    "\n",
    "def load_cifar10_train_dev(num_dev=100):\n",
    "  # randomly select and fixed for future (tracin-like strategy but their indices available only for mnist)\n",
    "  # selected_dev = np.random.randint(0, X_validation.shape[0], num_dev)\n",
    "  selected_dev = [5214, 2304, 5947, 9428, 2717, 8296, 7736, 8291, 5235, 54,\n",
    "                  7499, 9590, 3675, 1932, 6646, 8719, 6484, 6306, 3066, 2442,\n",
    "                  6106, 1949, 4320,  541, 1318, 5967, 2773, 3847, 1152, 9937,\n",
    "                  7469, 5982, 7644, 5820, 8152, 9518,  601, 3953, 4931, 1924,\n",
    "                  5342, 5467, 6718, 6779, 2860, 2440, 5480, 1178,  222, 7909,\n",
    "                  6394, 3511, 8729, 6261, 7192, 9453, 5257, 9077, 6419, 3280,\n",
    "                  3725, 3601, 8174, 5703, 4954, 9536, 4783, 2234, 7365, 2405,\n",
    "                  3073, 2780, 7461, 3525, 7573, 6764, 9962, 7527,  992,  315,\n",
    "                  6260, 9061,  592, 8003, 7594, 1930, 7215, 5124, 7531, 9471,\n",
    "                  2824, 3533, 6062, 3946, 5246, 4440,  414, 3572, 4899, 884]\n",
    "  X_train, Y_train, X_validation, Y_validation = load_standard_cifar10()\n",
    "  X_dev = X_validation[selected_dev]\n",
    "  Y_dev = Y_validation[selected_dev]\n",
    "  return X_train, Y_train, X_dev, Y_dev"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "background_save": true
    },
    "id": "_q_HiHPqg94u"
   },
   "outputs": [],
   "source": [
    "X_train, Y_train, X_test, Y_test = load_standard_cifar10()\n",
    "_, _, X_dev, Y_dev = load_cifar10_train_dev()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "background_save": true
    },
    "id": "OazEet6QmJaQ"
   },
   "outputs": [],
   "source": [
    "Y_train = np.squeeze(np.array(Y_train))\n",
    "Y_test = np.squeeze(np.array(Y_test))\n",
    "Y_dev = np.squeeze(np.array(Y_dev))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "background_save": true
    },
    "id": "jjRtFp3qlw8W",
    "outputId": "90c013a9-3475-40cd-c15d-99ae8293cbac"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(50000, 32, 32, 3) (10000, 32, 32, 3) (100, 32, 32, 3) (50000,) (10000,) (100,)\n"
     ]
    }
   ],
   "source": [
    "print(X_train.shape, X_test.shape, X_dev.shape, Y_train.shape, Y_test.shape, Y_dev.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Poisoning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "pct_poison = 1\n",
    "num_poison = int((pct_poison/100) * len(Y_train))\n",
    "ix_poison = np.random.choice(len(Y_train), num_poison, replace=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_train_dp = Y_train.copy()\n",
    "\n",
    "for i in ix_poison:\n",
    "    y_curr = Y_train[i]\n",
    "    y_new = np.random.choice([y for y in range(NUM_CLASSES) if y != y_curr])\n",
    "    Y_train_dp[i] = y_new\n",
    "\n",
    "ix_poisoned = np.zeros(len(Y_train)).astype(int)\n",
    "ix_poisoned[ix_poison] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# np.save(f\"{BASE_DIR}saved/{DATASET}_Y_train_dp_{pct_poison}pct.npy\", Y_train_dp)\n",
    "# np.save(f\"{BASE_DIR}saved/{DATASET}_ix_poisoned_{pct_poison}pct.npy\", ix_poisoned)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "c1aslxI4_kxI"
   },
   "source": [
    "## Model training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "background_save": true
    },
    "id": "4wtzw_Jq_lu1"
   },
   "outputs": [],
   "source": [
    "def dnn_custom(inp_dim, out_dim, dnn='resnet50', train_full=False, weights='imagenet', dropout_pct=0.25, use_upsampling=False):\n",
    "    inp_dim_orig = inp_dim\n",
    "    if use_upsampling:\n",
    "      inp_dim = (224,224,3)\n",
    "    if dnn=='resnet50':\n",
    "      dnn_model = tf.keras.applications.ResNet50(weights=weights, include_top=False, input_shape=inp_dim)\n",
    "    elif dnn=='mobilenet':\n",
    "      dnn_model = tf.keras.applications.MobileNet(weights=weights, include_top=False, input_shape=inp_dim)\n",
    "    elif dnn=='mobilenetv3':\n",
    "      dnn_model = tf.keras.applications.MobileNetV3Small(weights=weights, include_top=False, input_shape=inp_dim)\n",
    "    elif dnn=='efficientnetb0':\n",
    "      dnn_model = tf.keras.applications.EfficientNetB0(weights=weights, include_top=False, input_shape=inp_dim)\n",
    "    elif dnn=='efficientnetb3':\n",
    "      dnn_model = tf.keras.applications.EfficientNetB3(weights=weights, include_top=False, input_shape=inp_dim)\n",
    "    elif dnn=='efficientnetv2':\n",
    "      dnn_model = tf.keras.applications.EfficientNetV2L(weights=weights, include_top=False, input_shape=inp_dim)\n",
    "    elif dnn=='convnext':\n",
    "      dnn_model = tf.keras.applications.ConvNeXtBase(weights=weights, include_top=False, input_shape=inp_dim)\n",
    "    elif dnn=='xception':\n",
    "      dnn_model = tf.keras.applications.Xception(weights=weights, include_top=False, input_shape=inp_dim)\n",
    "\n",
    "    if not train_full:\n",
    "      for layer in dnn_model.layers:\n",
    "          if isinstance(layer, tf.keras.layers.BatchNormalization):\n",
    "              layer.trainable = True\n",
    "          else:\n",
    "              layer.trainable = False\n",
    "\n",
    "    model = tf.keras.models.Sequential()\n",
    "    model.add(tf.keras.layers.Input(shape=inp_dim_orig))\n",
    "    if use_upsampling & inp_dim_orig[0] != 224:\n",
    "      model.add(tf.keras.layers.UpSampling2D(size=(224/inp_dim_orig[0], 224/inp_dim_orig[0]), interpolation='bilinear'))\n",
    "    model.add(dnn_model)\n",
    "    model.add(tf.keras.layers.GlobalAveragePooling2D())\n",
    "    model.add(tf.keras.layers.Dropout(dropout_pct))\n",
    "    model.add(tf.keras.layers.Dense(out_dim*16, activation='relu'))\n",
    "    model.add(tf.keras.layers.BatchNormalization())\n",
    "    model.add(tf.keras.layers.Dense(out_dim, activation='softmax'))\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "pct_poison = 1\n",
    "# Y_train is the poisoned data (to require minimal changes in pipeline ahead)\n",
    "Y_train = np.load(f\"{BASE_DIR}/{DATASET}_Y_train_dp_{pct_poison}pct.npy\")\n",
    "ix_poisoned = np.load(f\"{BASE_DIR}/{DATASET}_ix_poisoned_{pct_poison}pct.npy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "xu76b-ds_l0d",
    "outputId": "fe108287-da7c-4b51-c069-0eb5a261eeff"
   },
   "outputs": [],
   "source": [
    "BASE_DNN = 'resnet50'\n",
    "EPOCHS = 1000\n",
    "saved_epochs = [1, 5, 10, 30, 50, 75, 100, 200, 300, 400, 500, 600, 700, 750, 800, 900, 1000]\n",
    "\n",
    "start_epoch = 0\n",
    "for e in saved_epochs:\n",
    "    try:\n",
    "        clf = tf.keras.models.load_model(f\"{BASE_DIR}/{DATASET}_{BASE_DNN}_{e}e_{pct_poison}dp.h5\")\n",
    "        start_epoch = e\n",
    "    except:\n",
    "        break\n",
    "    \n",
    "if start_epoch == 0:\n",
    "    clf = dnn_custom(inp_dim=(32,32,3), out_dim=NUM_CLASSES, dnn=BASE_DNN, dropout_pct=0, train_full=False, use_upsampling=True)\n",
    "    clf.compile(\n",
    "        optimizer='Adam', \n",
    "        loss=tf.keras.losses.SparseCategoricalCrossentropy(), \n",
    "        metrics=['accuracy', tf.keras.metrics.SparseTopKCategoricalAccuracy(k=5)]\n",
    "    )\n",
    "\n",
    "print(start_epoch)\n",
    "for i in range(start_epoch+1,EPOCHS+1):\n",
    "  if (i%10) == 0:\n",
    "    print(i)\n",
    "  clf.fit(X_train, Y_train, epochs=1, batch_size=BATCH_SIZE, validation_data=(X_test, Y_test), verbose=1)\n",
    "  if i in saved_epochs:\n",
    "    clf.save(f\"{BASE_DIR}/{DATASET}_{BASE_DNN}_{i}e_{pct_poison}dp.h5\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exploring alternate models (DNN architectures)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "efficientnetv2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-22 02:59:13.724313: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1886] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 78953 MB memory:  -> device: 0, name: NVIDIA A100 80GB PCIe, pci bus id: 0000:81:00.0, compute capability: 8.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "70\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-22 03:00:57.228636: E tensorflow/core/grappler/optimizers/meta_optimizer.cc:961] layout failed: INVALID_ARGUMENT: Size of values 0 does not match size of permutation 4 @ fanin shape insequential/efficientnetv2-l/block1b_drop/dropout/SelectV2-2-TransposeNHWCToNCHW-LayoutOptimizer\n",
      "2023-11-22 03:01:02.677142: I tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:442] Loaded cuDNN version 8902\n",
      "2023-11-22 03:01:09.846773: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x2b4f480032f0 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
      "2023-11-22 03:01:09.846837: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): NVIDIA A100 80GB PCIe, Compute Capability 8.0\n",
      "2023-11-22 03:01:09.853920: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:269] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\n",
      "2023-11-22 03:01:10.193291: I ./tensorflow/compiler/jit/device_compiler.h:186] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "391/391 [==============================] - 413s 919ms/step - loss: 0.0068 - accuracy: 0.9979 - sparse_top_k_categorical_accuracy: 1.0000 - val_loss: 0.1706 - val_accuracy: 0.9723 - val_sparse_top_k_categorical_accuracy: 0.9996\n",
      "391/391 [==============================] - 355s 909ms/step - loss: 0.0065 - accuracy: 0.9981 - sparse_top_k_categorical_accuracy: 1.0000 - val_loss: 0.1665 - val_accuracy: 0.9730 - val_sparse_top_k_categorical_accuracy: 0.9996\n",
      "391/391 [==============================] - 354s 904ms/step - loss: 0.0059 - accuracy: 0.9981 - sparse_top_k_categorical_accuracy: 1.0000 - val_loss: 0.1748 - val_accuracy: 0.9740 - val_sparse_top_k_categorical_accuracy: 0.9995\n",
      "391/391 [==============================] - 355s 907ms/step - loss: 0.0057 - accuracy: 0.9982 - sparse_top_k_categorical_accuracy: 1.0000 - val_loss: 0.1922 - val_accuracy: 0.9695 - val_sparse_top_k_categorical_accuracy: 0.9988\n",
      "391/391 [==============================] - 355s 909ms/step - loss: 0.0052 - accuracy: 0.9983 - sparse_top_k_categorical_accuracy: 1.0000 - val_loss: 0.1743 - val_accuracy: 0.9715 - val_sparse_top_k_categorical_accuracy: 0.9994\n",
      "391/391 [==============================] - 355s 908ms/step - loss: 0.0054 - accuracy: 0.9983 - sparse_top_k_categorical_accuracy: 1.0000 - val_loss: 0.1701 - val_accuracy: 0.9721 - val_sparse_top_k_categorical_accuracy: 0.9993\n",
      "391/391 [==============================] - 353s 902ms/step - loss: 0.0059 - accuracy: 0.9980 - sparse_top_k_categorical_accuracy: 1.0000 - val_loss: 0.1770 - val_accuracy: 0.9718 - val_sparse_top_k_categorical_accuracy: 0.9992\n",
      "391/391 [==============================] - 354s 905ms/step - loss: 0.0065 - accuracy: 0.9982 - sparse_top_k_categorical_accuracy: 1.0000 - val_loss: 0.1754 - val_accuracy: 0.9707 - val_sparse_top_k_categorical_accuracy: 0.9992\n",
      "391/391 [==============================] - 354s 905ms/step - loss: 0.0054 - accuracy: 0.9983 - sparse_top_k_categorical_accuracy: 1.0000 - val_loss: 0.1758 - val_accuracy: 0.9707 - val_sparse_top_k_categorical_accuracy: 0.9995\n",
      "80\n",
      "391/391 [==============================] - 353s 904ms/step - loss: 0.0053 - accuracy: 0.9984 - sparse_top_k_categorical_accuracy: 1.0000 - val_loss: 0.1689 - val_accuracy: 0.9726 - val_sparse_top_k_categorical_accuracy: 0.9995\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mahmood6/Desktop/projects/mdl/lib/python3.9/site-packages/keras/src/engine/training.py:3079: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
      "  saving_api.save_model(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "391/391 [==============================] - 353s 902ms/step - loss: 0.0060 - accuracy: 0.9983 - sparse_top_k_categorical_accuracy: 1.0000 - val_loss: 0.1674 - val_accuracy: 0.9726 - val_sparse_top_k_categorical_accuracy: 0.9996\n",
      "391/391 [==============================] - 353s 902ms/step - loss: 0.0057 - accuracy: 0.9982 - sparse_top_k_categorical_accuracy: 1.0000 - val_loss: 0.1710 - val_accuracy: 0.9724 - val_sparse_top_k_categorical_accuracy: 0.9996\n",
      "391/391 [==============================] - 353s 902ms/step - loss: 0.0048 - accuracy: 0.9983 - sparse_top_k_categorical_accuracy: 1.0000 - val_loss: 0.1809 - val_accuracy: 0.9722 - val_sparse_top_k_categorical_accuracy: 0.9994\n",
      "391/391 [==============================] - 353s 902ms/step - loss: 0.0051 - accuracy: 0.9982 - sparse_top_k_categorical_accuracy: 1.0000 - val_loss: 0.1805 - val_accuracy: 0.9732 - val_sparse_top_k_categorical_accuracy: 0.9996\n",
      "391/391 [==============================] - 354s 904ms/step - loss: 0.0052 - accuracy: 0.9985 - sparse_top_k_categorical_accuracy: 1.0000 - val_loss: 0.1850 - val_accuracy: 0.9714 - val_sparse_top_k_categorical_accuracy: 0.9995\n",
      "391/391 [==============================] - 354s 904ms/step - loss: 0.0064 - accuracy: 0.9979 - sparse_top_k_categorical_accuracy: 1.0000 - val_loss: 0.1632 - val_accuracy: 0.9719 - val_sparse_top_k_categorical_accuracy: 0.9988\n",
      "391/391 [==============================] - 354s 904ms/step - loss: 0.0051 - accuracy: 0.9985 - sparse_top_k_categorical_accuracy: 1.0000 - val_loss: 0.1794 - val_accuracy: 0.9728 - val_sparse_top_k_categorical_accuracy: 0.9994\n",
      "391/391 [==============================] - 353s 904ms/step - loss: 0.0047 - accuracy: 0.9984 - sparse_top_k_categorical_accuracy: 1.0000 - val_loss: 0.1798 - val_accuracy: 0.9728 - val_sparse_top_k_categorical_accuracy: 0.9994\n",
      "391/391 [==============================] - 354s 904ms/step - loss: 0.0048 - accuracy: 0.9985 - sparse_top_k_categorical_accuracy: 1.0000 - val_loss: 0.1643 - val_accuracy: 0.9740 - val_sparse_top_k_categorical_accuracy: 0.9991\n",
      "90\n",
      "391/391 [==============================] - 353s 903ms/step - loss: 0.0055 - accuracy: 0.9982 - sparse_top_k_categorical_accuracy: 1.0000 - val_loss: 0.1717 - val_accuracy: 0.9717 - val_sparse_top_k_categorical_accuracy: 0.9989\n",
      "391/391 [==============================] - 354s 904ms/step - loss: 0.0047 - accuracy: 0.9984 - sparse_top_k_categorical_accuracy: 1.0000 - val_loss: 0.1676 - val_accuracy: 0.9739 - val_sparse_top_k_categorical_accuracy: 0.9993\n",
      "391/391 [==============================] - 353s 903ms/step - loss: 0.0047 - accuracy: 0.9984 - sparse_top_k_categorical_accuracy: 1.0000 - val_loss: 0.1793 - val_accuracy: 0.9721 - val_sparse_top_k_categorical_accuracy: 0.9995\n",
      "391/391 [==============================] - 353s 903ms/step - loss: 0.0068 - accuracy: 0.9979 - sparse_top_k_categorical_accuracy: 1.0000 - val_loss: 0.1721 - val_accuracy: 0.9724 - val_sparse_top_k_categorical_accuracy: 0.9991\n",
      "391/391 [==============================] - 353s 903ms/step - loss: 0.0066 - accuracy: 0.9979 - sparse_top_k_categorical_accuracy: 1.0000 - val_loss: 0.1694 - val_accuracy: 0.9744 - val_sparse_top_k_categorical_accuracy: 0.9992\n",
      "391/391 [==============================] - 354s 904ms/step - loss: 0.0041 - accuracy: 0.9987 - sparse_top_k_categorical_accuracy: 1.0000 - val_loss: 0.1730 - val_accuracy: 0.9740 - val_sparse_top_k_categorical_accuracy: 0.9995\n",
      "391/391 [==============================] - 353s 904ms/step - loss: 0.0049 - accuracy: 0.9985 - sparse_top_k_categorical_accuracy: 1.0000 - val_loss: 0.1834 - val_accuracy: 0.9740 - val_sparse_top_k_categorical_accuracy: 0.9991\n",
      "391/391 [==============================] - 353s 903ms/step - loss: 0.0039 - accuracy: 0.9986 - sparse_top_k_categorical_accuracy: 1.0000 - val_loss: 0.1758 - val_accuracy: 0.9718 - val_sparse_top_k_categorical_accuracy: 0.9990\n",
      "391/391 [==============================] - 353s 904ms/step - loss: 0.0040 - accuracy: 0.9988 - sparse_top_k_categorical_accuracy: 1.0000 - val_loss: 0.1675 - val_accuracy: 0.9743 - val_sparse_top_k_categorical_accuracy: 0.9995\n",
      "391/391 [==============================] - 355s 907ms/step - loss: 0.0045 - accuracy: 0.9984 - sparse_top_k_categorical_accuracy: 1.0000 - val_loss: 0.1818 - val_accuracy: 0.9728 - val_sparse_top_k_categorical_accuracy: 0.9994\n",
      "100\n",
      "391/391 [==============================] - 355s 907ms/step - loss: 0.0039 - accuracy: 0.9987 - sparse_top_k_categorical_accuracy: 1.0000 - val_loss: 0.1875 - val_accuracy: 0.9727 - val_sparse_top_k_categorical_accuracy: 0.9992\n"
     ]
    }
   ],
   "source": [
    "base_dnn_list = ['efficientnetv2'] #, 'efficientnetb0', 'efficientnetb3', 'convnext']\n",
    "EPOCHS = 100\n",
    "saved_epochs = [1, 5, 10, 20, 30, 40, 50, 60, 70, 80, 90, 100]\n",
    "\n",
    "for dnn in base_dnn_list:\n",
    "    print(dnn)\n",
    "    start_epoch = 0\n",
    "    for e in saved_epochs:\n",
    "        try:\n",
    "            # custom layer object \n",
    "            if dnn == 'convnext':\n",
    "                clf = dnn_custom(inp_dim=(32,32,3), out_dim=NUM_CLASSES, dnn=dnn, dropout_pct=0, train_full=False, use_upsampling=True)\n",
    "                clf.compile(\n",
    "                    optimizer='Adam', \n",
    "                    loss=tf.keras.losses.SparseCategoricalCrossentropy(), \n",
    "                    metrics=['accuracy', tf.keras.metrics.SparseTopKCategoricalAccuracy(k=5)]\n",
    "                )\n",
    "                clf.load_weights(f\"{BASE_DIR}/{DATASET}_{dnn}_{e}e_{pct_poison}dp.h5\")\n",
    "            else:\n",
    "                clf = tf.keras.models.load_model(f\"{BASE_DIR}/{DATASET}_{dnn}_{e}e_{pct_poison}dp.h5\")\n",
    "            start_epoch = e\n",
    "        except:\n",
    "            break\n",
    "\n",
    "    if start_epoch == 0:\n",
    "        clf = dnn_custom(inp_dim=(32,32,3), out_dim=NUM_CLASSES, dnn=dnn, dropout_pct=0, train_full=False, use_upsampling=True)\n",
    "        clf.compile(\n",
    "            optimizer='Adam', \n",
    "            loss=tf.keras.losses.SparseCategoricalCrossentropy(), \n",
    "            metrics=['accuracy', tf.keras.metrics.SparseTopKCategoricalAccuracy(k=5)]\n",
    "        )\n",
    "\n",
    "    print(start_epoch)\n",
    "    for i in range(start_epoch+1,EPOCHS+1):\n",
    "      if (i%10) == 0:\n",
    "        print(i)\n",
    "      clf.fit(X_train, Y_train, epochs=1, batch_size=BATCH_SIZE, validation_data=(X_test, Y_test), verbose=1)\n",
    "      if i in saved_epochs:\n",
    "        clf.save(f\"{BASE_DIR}/{DATASET}_{dnn}_{i}e_{pct_poison}dp.h5\")"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "machine_shape": "hm",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "mdl",
   "language": "python",
   "name": "mdl"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
