{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "KqbFELV9m40B"
   },
   "outputs": [],
   "source": [
    "import tensorflow.compat.v1 as tf\n",
    "from tensorflow import keras\n",
    "import numpy as np\n",
    "import random\n",
    "import os\n",
    "import gc\n",
    "import time\n",
    "import platform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "bwASQ3ySmWrL",
    "outputId": "2bc83610-b665-443f-df44-f393d36be596"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/tensorflow/python/compat/v2_compat.py:107: disable_resource_variables (from tensorflow.python.ops.variable_scope) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "non-resource variables are not supported in the long term\n"
     ]
    }
   ],
   "source": [
    "tf.disable_v2_behavior()\n",
    "\n",
    "from tensorflow.compat.v1.keras import backend as K\n",
    "sess = tf.Session()\n",
    "K.set_session(sess)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XrjryHpVnCNK"
   },
   "outputs": [],
   "source": [
    "NUM_CLASSES = 10\n",
    "WIDTH = 32\n",
    "NUM_CHANNELS = 3\n",
    "\n",
    "DATASET = \"cifar10\"\n",
    "BASE_DIR = f\"\"\n",
    "\n",
    "TRIAL = 1\n",
    "MODEL_DIR = f\"{BASE_DIR}/tmp/cifar10/original/{TRIAL}/model_ckpt_11.h5\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dCaiV_qeqge8"
   },
   "source": [
    "# Data loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "A7kqRlXJQz3s"
   },
   "outputs": [],
   "source": [
    "def _normalize(X):\n",
    "  assert X.dtype == np.uint8\n",
    "  X = X.astype(np.float64)\n",
    "  X /= 255\n",
    "  return X\n",
    "\n",
    "def get_one_hot(targets, nb_classes):\n",
    "  res = np.eye(nb_classes)[np.array(targets).reshape(-1)]\n",
    "  return res.reshape(list(targets.shape)+[nb_classes])\n",
    "\n",
    "def load_standard_cifar10():\n",
    "  (X_train, Y_train), (X_validation, Y_validation) = tf.keras.datasets.cifar10.load_data()\n",
    "  X_train = X_train.reshape(X_train.shape[0], WIDTH, WIDTH, NUM_CHANNELS)\n",
    "  X_validation = X_validation.reshape(X_validation.shape[0], WIDTH, WIDTH, NUM_CHANNELS)\n",
    "\n",
    "  X_train = _normalize(X_train)\n",
    "  X_validation = _normalize(X_validation)\n",
    "\n",
    "  Y_train = Y_train.astype(np.int32)\n",
    "  Y_validation = Y_validation.astype(np.int32)\n",
    "\n",
    "  return X_train, Y_train, X_validation, Y_validation\n",
    "\n",
    "def load_cifar10_train_dev(num_dev=100):\n",
    "  # randomly select and fixed for future (tracin-like strategy but their indices available only for mnist)\n",
    "  # selected_dev = np.random.randint(0, X_validation.shape[0], num_dev)\n",
    "  selected_dev = [5214, 2304, 5947, 9428, 2717, 8296, 7736, 8291, 5235, 54,\n",
    "                  7499, 9590, 3675, 1932, 6646, 8719, 6484, 6306, 3066, 2442,\n",
    "                  6106, 1949, 4320,  541, 1318, 5967, 2773, 3847, 1152, 9937,\n",
    "                  7469, 5982, 7644, 5820, 8152, 9518,  601, 3953, 4931, 1924,\n",
    "                  5342, 5467, 6718, 6779, 2860, 2440, 5480, 1178,  222, 7909,\n",
    "                  6394, 3511, 8729, 6261, 7192, 9453, 5257, 9077, 6419, 3280,\n",
    "                  3725, 3601, 8174, 5703, 4954, 9536, 4783, 2234, 7365, 2405,\n",
    "                  3073, 2780, 7461, 3525, 7573, 6764, 9962, 7527,  992,  315,\n",
    "                  6260, 9061,  592, 8003, 7594, 1930, 7215, 5124, 7531, 9471,\n",
    "                  2824, 3533, 6062, 3946, 5246, 4440,  414, 3572, 4899, 884]\n",
    "  X_train, Y_train, X_validation, Y_validation = load_standard_cifar10()\n",
    "  X_dev = X_validation[selected_dev]\n",
    "  Y_dev = Y_validation[selected_dev]\n",
    "  return X_train, Y_train, X_dev, Y_dev"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "W_zklsWaSfOe",
    "outputId": "91561ddb-5218-4d60-e667-09cb10301ffe"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz\n",
      "170500096/170498071 [==============================] - 4s 0us/step\n",
      "170508288/170498071 [==============================] - 4s 0us/step\n"
     ]
    }
   ],
   "source": [
    "X_train, Y_train, X_dev, Y_dev = load_cifar10_train_dev()\n",
    "Y_train = get_one_hot(Y_train, NUM_CLASSES)\n",
    "Y_dev = get_one_hot(Y_dev, NUM_CLASSES)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "AxbHgFkeV4R7"
   },
   "outputs": [],
   "source": [
    "Y_train = np.squeeze(Y_train)\n",
    "Y_dev = np.squeeze(Y_dev)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZonLT6kfqV0i"
   },
   "source": [
    "# Model loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "KWnk-esibhaL",
    "outputId": "651fc857-d942-49b8-de3c-1a3a0ee26f42"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Error in loading the saved optimizer state. As a result, your model is starting with a freshly initialized optimizer.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " conv2d_6 (Conv2D)           (None, 32, 32, 32)        896       \n",
      "                                                                 \n",
      " conv2d_7 (Conv2D)           (None, 32, 32, 32)        9248      \n",
      "                                                                 \n",
      " max_pooling2d_3 (MaxPooling  (None, 16, 16, 32)       0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " dropout_4 (Dropout)         (None, 16, 16, 32)        0         \n",
      "                                                                 \n",
      " conv2d_8 (Conv2D)           (None, 16, 16, 64)        18496     \n",
      "                                                                 \n",
      " conv2d_9 (Conv2D)           (None, 16, 16, 64)        36928     \n",
      "                                                                 \n",
      " max_pooling2d_4 (MaxPooling  (None, 8, 8, 64)         0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " dropout_5 (Dropout)         (None, 8, 8, 64)          0         \n",
      "                                                                 \n",
      " conv2d_10 (Conv2D)          (None, 8, 8, 128)         73856     \n",
      "                                                                 \n",
      " conv2d_11 (Conv2D)          (None, 8, 8, 128)         147584    \n",
      "                                                                 \n",
      " max_pooling2d_5 (MaxPooling  (None, 4, 4, 128)        0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " dropout_6 (Dropout)         (None, 4, 4, 128)         0         \n",
      "                                                                 \n",
      " flatten_1 (Flatten)         (None, 2048)              0         \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 128)               262272    \n",
      "                                                                 \n",
      " dropout_7 (Dropout)         (None, 128)               0         \n",
      "                                                                 \n",
      " dense_3 (Dense)             (None, 10)                1290      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 550,570\n",
      "Trainable params: 550,570\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = keras.models.load_model(MODEL_DIR)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jDno9rcnkYR-"
   },
   "source": [
    "# Influence functions implemention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JIZnOneKiPEF"
   },
   "outputs": [],
   "source": [
    "from tensorflow.compat.v1.keras import layers\n",
    "\n",
    "\n",
    "def flatten(tensor_list):\n",
    "  \"\"\"Flattens a list of tensor into a single rank-1 tensor.\"\"\"\n",
    "  return tf.concat([tf.reshape(x, [-1]) for x in tensor_list], 0)\n",
    "\n",
    "\n",
    "class InfSketchLayer(layers.Layer):\n",
    "  \"\"\"A Keras layer encapsulating the influence computation.\"\"\"\n",
    "\n",
    "  def __init__(self,\n",
    "               base_params_fn,\n",
    "               base_model_num_params,\n",
    "               target=None,\n",
    "               sketch_size=100):\n",
    "    \"\"\"Constructs the InfSketchLayer.\n",
    "\n",
    "    Args:\n",
    "      base_params_fn: A callable returning a list of the base model parameters.\n",
    "      base_model_num_params: The total number of base model parameters.\n",
    "      target: An optional matrix of shape (sketch_size, base_model_num_params)\n",
    "        that will be the target matrix for the sketch computation. The sketch\n",
    "        computed will be Hessian^{-1} * target.\n",
    "      sketch_size: the size of the sketch. Larger values give\n",
    "        better accuracy but involve slower computation.\n",
    "    \"\"\"\n",
    "    super(InfSketchLayer, self).__init__()\n",
    "    self._sketch_size = sketch_size\n",
    "    self._base_params_fn = base_params_fn\n",
    "    if target is None:\n",
    "      self.target = self.add_weight(\n",
    "          name='target',\n",
    "          shape=(self._sketch_size, base_model_num_params),\n",
    "          dtype=tf.float32,\n",
    "          initializer=tf.keras.initializers.random_normal(stddev=1.0 /\n",
    "                                                          np.sqrt(sketch_size)),\n",
    "          trainable=False)\n",
    "    else:\n",
    "      self.target = target\n",
    "      self._sketch_size = target.shape[0]\n",
    "    self.sketch = self.add_weight(\n",
    "        name='sketch',\n",
    "        shape=self.target.shape,\n",
    "        initializer='zeros',\n",
    "        trainable=True)\n",
    "\n",
    "  def call(self, inputs, training=True):\n",
    "    \"\"\"inputs is expected to be the gradient of the loss.\"\"\"\n",
    "    if training:\n",
    "      grad1_x_sketch = tf.matmul(self.sketch, tf.reshape(inputs[0], [-1, 1]))\n",
    "      grad2_x_sketch = tf.matmul(self.sketch, tf.reshape(inputs[1], [-1, 1]))\n",
    "      base_params = self._base_params_fn()\n",
    "\n",
    "      def loss_i(i):\n",
    "        # si = self.sketch[i]\n",
    "        ti = self.target[i]\n",
    "        gs1 = grad1_x_sketch[i][0]\n",
    "        hs1 = tf.cast(flatten(tf.gradients(gs1, base_params)), tf.float32)\n",
    "        gs2 = grad2_x_sketch[i][0]\n",
    "        hs2 = tf.cast(flatten(tf.gradients(gs2, base_params)), tf.float32)\n",
    "        return tf.tensordot(hs1 - ti, hs2 - ti, 1)\n",
    "\n",
    "      _, loss = tf.while_loop(\n",
    "          lambda i, _: i < self._sketch_size,\n",
    "          lambda i, loss: (i + 1, loss + loss_i(i)),\n",
    "          [tf.constant(0, tf.int32),\n",
    "           tf.constant(0.0, tf.float32)])\n",
    "      output = tf.reshape(loss, [1, -1])\n",
    "    else:\n",
    "      grad_x_sketch = tf.matmul(self.sketch, inputs)\n",
    "      output = tf.matmul(tf.transpose(self.target), grad_x_sketch)\n",
    "    return output\n",
    "\n",
    "\n",
    "class LossGradientLayer(layers.Layer):\n",
    "  \"\"\"A Keras layer computing the gradient of the loss.\"\"\"\n",
    "\n",
    "  def __init__(self, base_model_fn):\n",
    "    super(LossGradientLayer, self).__init__()\n",
    "    self._base_model_fn = base_model_fn\n",
    "\n",
    "  def call(self, inputs):\n",
    "    base_loss, base_params = self._base_model_fn(inputs)\n",
    "    return tf.cast(flatten(tf.gradients(base_loss, base_params)), tf.float32)\n",
    "\n",
    "\n",
    "def _identity_loss(_, y_pred):\n",
    "  \"\"\"Defines the identity loss function.\"\"\"\n",
    "  return y_pred\n",
    "\n",
    "\n",
    "def _base_model_fns(base_model, loss_fn):\n",
    "  \"\"\"Helper function to construct base_model_fn and base_params_fn.\"\"\"\n",
    "  def base_model_fn(inputs):\n",
    "    features = inputs[0]\n",
    "    labels = inputs[1]\n",
    "    preds = base_model(features, training=False)\n",
    "    return loss_fn(labels, preds), base_model.trainable_variables\n",
    "\n",
    "  def base_params_fn():\n",
    "    return base_model.trainable_variables\n",
    "\n",
    "  return base_model_fn, base_params_fn\n",
    "\n",
    "\n",
    "def test_loss_grads(base_model, loss_fn, test_inputs):\n",
    "  \"\"\"Computes the loss gradient at the given test points.\"\"\"\n",
    "  num_points = test_inputs[0].shape[0]\n",
    "  features_input = tf.keras.Input(shape=test_inputs[0].shape[1:])\n",
    "  labels_input = tf.keras.Input(shape=test_inputs[1].shape[1:])\n",
    "  base_model_fn, _ = _base_model_fns(base_model, loss_fn)\n",
    "  test_grad = tf.reshape(\n",
    "      LossGradientLayer(base_model_fn)([features_input, labels_input]), [1, -1])\n",
    "  m = tf.keras.Model(inputs=[features_input, labels_input], outputs=test_grad)\n",
    "  grads = np.zeros((num_points, base_model.count_params()), dtype=np.float32)\n",
    "  for i in range(num_points):\n",
    "    grads[i] = m.predict([[test_inputs[0][i]], [test_inputs[1][i]]])[0]\n",
    "  return grads\n",
    "\n",
    "\n",
    "class InfSketcher(object):\n",
    "  \"\"\"A helper class encapsulating two models: sketcher_model and inf_model.\n",
    "\n",
    "  sketcher_model: computes the inverse Hessian sketch\n",
    "  inf_model: computes influences, accessed via the inf_model() function.\n",
    "  \"\"\"\n",
    "\n",
    "  def __init__(self,\n",
    "               base_model,\n",
    "               inputs,\n",
    "               loss_fn,\n",
    "               target=None,\n",
    "               sketch_size=100,\n",
    "               optimizer='sgd',\n",
    "               sketch_path=None):\n",
    "    base_model_fn, base_params_fn = _base_model_fns(base_model, loss_fn)\n",
    "    self._grad_layer = LossGradientLayer(base_model_fn)\n",
    "    f0, f1 = tf.split(inputs[0], 2)\n",
    "    l0, l1 = tf.split(inputs[1], 2)\n",
    "    grad1 = self._grad_layer([f0, l0])\n",
    "    grad2 = self._grad_layer([f1, l1])\n",
    "    self._is_layer = InfSketchLayer(base_params_fn, base_model.count_params(),\n",
    "                                    target, sketch_size)\n",
    "    train_loss = self._is_layer([grad1, grad2], training=True)\n",
    "    self.sketcher_model = tf.keras.Model(\n",
    "        inputs=inputs, outputs=train_loss)\n",
    "    self.sketcher_model.compile(optimizer=optimizer, loss=_identity_loss)\n",
    "    if sketch_path is not None:\n",
    "      self.sketcher_model.load_weights(sketch_path)\n",
    "\n",
    "  def get_inf_model(self, test_input, train_inputs):\n",
    "    \"\"\"Influence computing model using a single test point as target.\"\"\"\n",
    "    inf_grad = self._grad_layer(train_inputs)\n",
    "    test_grad = self._grad_layer(test_input)\n",
    "    invhess_x_test_grad = self._is_layer(\n",
    "        tf.reshape(test_grad, [-1, 1]), training=False)\n",
    "    influence = tf.reshape(tf.tensordot(inf_grad, invhess_x_test_grad, 1), [-1])\n",
    "    self.inf_model = tf.keras.Model(\n",
    "        inputs=train_inputs, outputs=influence)\n",
    "    return self.inf_model\n",
    "\n",
    "  def get_grads_inf_model(self, test_grads, train_inputs):\n",
    "    \"\"\"Influence computing model for target/test grads using the sketched inv hessian.\"\"\"\n",
    "\n",
    "    inf_grad = self._grad_layer(train_inputs)  # shape = [num_params]\n",
    "    invhess_x_test_grads = self._is_layer(\n",
    "        tf.transpose(test_grads),\n",
    "        training=False)  # shape = [num_params, num_test_inputs]\n",
    "    # influence of this training point over the test_grads provided.\n",
    "    influence = tf.reshape(\n",
    "        tf.matmul(tf.reshape(inf_grad, [1, -1]), invhess_x_test_grads),\n",
    "        [1, -1])  # shape = [num_test_inputs]\n",
    "    self.inf_model = tf.keras.Model(inputs=train_inputs, outputs=influence)\n",
    "    return self.inf_model\n",
    "\n",
    "  def target_inf_model(self, train_inputs):\n",
    "    inf_grad = self._grad_layer(train_inputs)\n",
    "    sketch_x_grad = tf.reshape(\n",
    "        tf.matmul(self._is_layer.sketch, tf.reshape(inf_grad, [-1, 1])),\n",
    "        [1, -1])\n",
    "    self.target_inf_model = tf.keras.Model(\n",
    "        inputs=train_inputs, outputs=sketch_x_grad)\n",
    "    return self.target_inf_model\n",
    "\n",
    "  def save(self, path):\n",
    "    self.sketcher_model.save_weights(path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cc55-GNM08--"
   },
   "source": [
    "# Influence on loss of test points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "DoXo-6O9e_3R"
   },
   "outputs": [],
   "source": [
    "test_points = [X_dev, Y_dev]\n",
    "test_grads = test_loss_grads(model, tf.keras.losses.CategoricalCrossentropy(), test_points)\n",
    "test_grads = tf.constant(test_grads)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "V2d9R2knkW-R"
   },
   "outputs": [],
   "source": [
    "features_input = tf.keras.Input(shape=(WIDTH, WIDTH, NUM_CHANNELS))\n",
    "labels_input = tf.keras.Input(shape=(NUM_CLASSES,))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "f4DAK5XRMZL_",
    "outputId": "3292975a-183a-4566-cf9f-f51b54574a23"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sketch_path file does not exist, training a sketch\n"
     ]
    }
   ],
   "source": [
    "sketch_path = BASE_DIR + '/tmp/cifar10/original/{0}/sketcher_model.h5'.format(TRIAL)\n",
    "if os.path.exists(sketch_path):\n",
    "  print('using sketch from {0}'.format(sketch_path))\n",
    "  my_is = InfSketcher(model,\n",
    "                                [features_input, labels_input],\n",
    "                                tf.keras.losses.CategoricalCrossentropy(),\n",
    "                                target=test_grads,\n",
    "                                optimizer='adam',\n",
    "                                sketch_path=sketch_path,\n",
    "                      sketch_size=20)  # temp; to account for memory\n",
    "  sketcher_model = my_is.sketcher_model\n",
    "else:\n",
    "  print('sketch_path file does not exist, training a sketch')\n",
    "  my_is = InfSketcher(model,\n",
    "                                [features_input, labels_input],\n",
    "                                tf.keras.losses.CategoricalCrossentropy(),\n",
    "                                optimizer='adam',\n",
    "                                target=test_grads)\n",
    "  sketcher_model = my_is.sketcher_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2K1oVs-Xqk0g"
   },
   "outputs": [],
   "source": [
    "for i in range(8):\n",
    "  print('round {}'.format(i))\n",
    "  # sketcher_model.fit(x=[X_train, Y_train], y=None, verbose=1, epochs=10, batch_size=10000)\n",
    "  sketcher_model.fit(x=[X_train, Y_train], y=None, verbose=1, epochs=5, batch_size=32)\n",
    "  sketcher_model.save_weights(sketch_path)\n",
    "print('saved sketch to {0}'.format(sketch_path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "a_Z5JtoBiBiY",
    "outputId": "78ff3720-e11e-44d4-b414-17aa99eb2db0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_6\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_7 (InputLayer)           [(None, 32, 32, 3)]  0           []                               \n",
      "                                                                                                  \n",
      " input_8 (InputLayer)           [(None, 10)]         0           []                               \n",
      "                                                                                                  \n",
      " tf_op_layer_split_6 (TensorFlo  [(None, 32, 32, 3),  0          ['input_7[0][0]']                \n",
      " wOpLayer)                       (None, 32, 32, 3)]                                               \n",
      "                                                                                                  \n",
      " tf_op_layer_split_7 (TensorFlo  [(None, 10),        0           ['input_8[0][0]']                \n",
      " wOpLayer)                       (None, 10)]                                                      \n",
      "                                                                                                  \n",
      " loss_gradient_layer_5 (LossGra  (550570,)           0           ['tf_op_layer_split_6[0][0]',    \n",
      " dientLayer)                                                      'tf_op_layer_split_7[0][0]',    \n",
      "                                                                  'tf_op_layer_split_6[0][1]',    \n",
      "                                                                  'tf_op_layer_split_7[0][1]']    \n",
      "                                                                                                  \n",
      " inf_sketch_layer_3 (InfSketchL  (1, 1)              55057000    ['loss_gradient_layer_5[0][0]',  \n",
      " ayer)                                                            'loss_gradient_layer_5[1][0]']  \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 55,057,000\n",
      "Trainable params: 55,057,000\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "my_is.sketcher_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cI8739PzWvNi"
   },
   "outputs": [],
   "source": [
    "NUM_TRAIN = Y_train.shape[0]\n",
    "NUM_DEV = Y_dev.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "xvYPW35hFnvF",
    "outputId": "14e0f5c2-5fac-49b5-94a0-b487923f9150"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "10000\n",
      "20000\n",
      "30000\n",
      "40000\n"
     ]
    }
   ],
   "source": [
    "target_inf_model = my_is.target_inf_model([features_input, labels_input])\n",
    "# influences = np.zeros(shape=(60000,100), dtype=np.float32)\n",
    "influences = np.zeros(shape=(NUM_TRAIN,NUM_DEV), dtype=np.float32)\n",
    "for i in range(NUM_TRAIN):\n",
    "  if (i % 10000) == 0:\n",
    "    print(i)\n",
    "  influences[i] = my_is.target_inf_model.predict([[X_train[i]], [Y_train[i]]])[0]\n",
    "# np.savez_compressed('/tmp/ckpts/{0}/dev_influences_inf_fun.npz'.format(TRIAL), influences)\n",
    "np.savez_compressed(BASE_DIR + '/tmp/cifar10/original/{0}/dev_influences_inf_fun.npz'.format(TRIAL), influences)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "machine_shape": "hm",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3.9 (Default)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
